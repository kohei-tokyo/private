{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-22T11:38:56.415145Z",
     "start_time": "2025-10-22T11:38:55.413828Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from make_dataset import DatasetDigitalStaining\n",
    "\n",
    "dir = r\"D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG\"\n",
    "train_folders = [\"2\", \"3\", \"4\"]\n",
    "val_folders = [\"1\"]\n",
    "test_folders = [\"5\"]\n",
    "img_folders = [os.path.join(dir, f) for f in train_folders]\n",
    "train_datasets = [DatasetDigitalStaining(img_folders[i], augmentation=None) for i in range(len(train_folders))]\n",
    "val_datasets = [DatasetDigitalStaining(img_folders[i], augmentation=None) for i in range(len(val_folders))]\n",
    "\n",
    "for ph1,ph2, mito in train_datasets[0]:\n",
    "    print(ph1.shape, ph2.shape, mito.shape)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1224]) torch.Size([1, 1024, 1224]) torch.Size([1, 1024, 1224])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:38:56.430018Z",
     "start_time": "2025-10-22T11:38:56.416001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for ph1,ph2, mito in train_datasets[0]:\n",
    "    print(ph1.shape, ph2.shape, mito.shape)\n",
    "    \n",
    "    print(ph1.max(), ph2.max(), mito.max())\n",
    "    break"
   ],
   "id": "1d6428b980a62d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1224]) torch.Size([1, 1024, 1224]) torch.Size([1, 1024, 1224])\n",
      "tensor(0.0119) tensor(0.3455) tensor(19787.)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:38:57.047267Z",
     "start_time": "2025-10-22T11:38:56.430599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image # Import PIL\n",
    "\n",
    "def scale_to_uint8(img_np, p_min, p_max):\n",
    "    # 1. Clip the image to the global min/max range\n",
    "    img_clipped = np.clip(img_np, p_min, p_max)\n",
    "    \n",
    "    # 2. Normalize to [0, 1]\n",
    "    img_normalized = (img_clipped - p_min) / (p_max - p_min)\n",
    "    \n",
    "    # 3. Scale to [0, 255] and convert to uint8\n",
    "    img_uint8 = (img_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    return img_uint8\n",
    "\n",
    "def process_dataset(datasets_list, split_name, num_patches_per_image, patch_size, \n",
    "                    base_dir, prompt, ph1_stats, mito_stats):\n",
    "    \"\"\"\n",
    "    Iterates over a list of datasets, extracts random patches,\n",
    "    scales them to uint8 using GLOBAL stats, and saves as PNG.\n",
    "    \n",
    "    Args:\n",
    "        ...\n",
    "        ph1_stats (tuple): (global_p01, global_p99_9) for ph1\n",
    "        mito_stats (tuple): (global_p01, global_p99_9) for mito\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path = os.path.join(base_dir, split_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    metadata = []\n",
    "    patch_counter = 0\n",
    "    \n",
    "    ph1_p_min, ph1_p_max = ph1_stats\n",
    "    mito_p_min, mito_p_max = mito_stats\n",
    "    \n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    \n",
    "    for dataset in datasets_list:\n",
    "        for ph1, ph2, mito in tqdm(dataset):\n",
    "            \n",
    "            if ph1.dim() != 3 or mito.dim() != 3:\n",
    "                print(f\"Skipping image: unexpected dimensions. Got {ph1.shape} and {mito.shape}\")\n",
    "                continue\n",
    "                \n",
    "            _, h, w = ph1.shape\n",
    "            \n",
    "            if ph1.shape[1:] != mito.shape[1:]:\n",
    "                print(f\"Skipping image: ph1 and mito shape mismatch. {ph1.shape[1:]} vs {mito.shape[1:]}\")\n",
    "                continue\n",
    "                \n",
    "            if h < patch_size or w < patch_size:\n",
    "                print(f\"Skipping image: image is smaller than patch size. {h}x{w} vs {patch_size}\")\n",
    "                continue\n",
    "\n",
    "            for _ in range(num_patches_per_image):\n",
    "                y = torch.randint(0, h - patch_size + 1, (1,)).item()\n",
    "                x = torch.randint(0, w - patch_size + 1, (1,)).item()\n",
    "                \n",
    "                ph1_patch = ph1[:, y : y + patch_size, x : x + patch_size]\n",
    "                mito_patch = mito[:, y : y + patch_size, x : x + patch_size]\n",
    "                \n",
    "                ph1_np = ph1_patch.squeeze(0).numpy()\n",
    "                mito_np = mito_patch.squeeze(0).numpy()\n",
    "                \n",
    "                # --- Apply global scaling ---\n",
    "                ph1_uint8 = scale_to_uint8(ph1_np, ph1_p_min, ph1_p_max)\n",
    "                mito_uint8 = scale_to_uint8(mito_np, mito_p_min, mito_p_max)\n",
    "                \n",
    "                # --- Convert to PIL Image ---\n",
    "                # 'L' mode is for 8-bit grayscale\n",
    "                ph1_pil = Image.fromarray(ph1_uint8, mode='L')\n",
    "                mito_pil = Image.fromarray(mito_uint8, mode='L')\n",
    "\n",
    "                # 4. Define filenames with .png extension\n",
    "                base_filename = f\"{patch_counter:06d}\"\n",
    "                condition_filename = f\"{base_filename}_condition.png\"\n",
    "                target_filename = f\"{base_filename}_target.png\"\n",
    "                \n",
    "                condition_path = os.path.join(output_path, condition_filename)\n",
    "                target_path = os.path.join(output_path, target_filename)\n",
    "                \n",
    "                # 5. Save the images as PNGs\n",
    "                ph1_pil.save(condition_path)\n",
    "                mito_pil.save(target_path)\n",
    "                \n",
    "                # 6. Add entry to metadata\n",
    "                metadata.append({\n",
    "                    \"file_name\": target_filename,\n",
    "                    \"conditioning_image\": condition_filename,\n",
    "                    \"text\": prompt\n",
    "                })\n",
    "                \n",
    "                patch_counter += 1\n",
    "\n",
    "    # --- Save Metadata File ---\n",
    "    metadata_path = os.path.join(output_path, \"metadata.jsonl\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for entry in metadata:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "            \n",
    "    print(f\"\\nFinished processing {split_name} split.\")\n",
    "    print(f\"Saved {len(metadata)} patches to: {output_path}\")\n",
    "    print(f\"Saved metadata file to: {metadata_path}\\n\")\n",
    "\n",
    "    # --- Save Metadata File ---\n",
    "    metadata_path = os.path.join(output_path, \"metadata.jsonl\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for entry in metadata:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "            \n",
    "    print(f\"\\nFinished processing {split_name} split.\")\n",
    "    print(f\"Saved {len(metadata)} patches to: {output_path}\")\n",
    "    print(f\"Saved metadata file to: {metadata_path}\\n\")"
   ],
   "id": "8e50c7ea200efa7b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:39:01.902768Z",
     "start_time": "2025-10-22T11:38:57.047851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- Parameters to configure ---\n",
    "BASE_OUTPUT_DIR = \"..//controlnet_dataset_uint8\" # New output dir\n",
    "PATCH_SIZE = 512\n",
    "NUM_PATCHES_PER_IMAGE = 1\n",
    "PROMPT = \"fluorescence microscopy image of mitochondria\"\n",
    "\n",
    "# Percentiles to use for normalization\n",
    "# (0.1, 99.9) is a good start, ignores 0.2% of outliers\n",
    "P_LOW = 0.1 \n",
    "P_HIGH = 99.9\n",
    "\n",
    "BASE_DATA_DIR = dir\n",
    "\n",
    "# --- Your Dataset Initialization Code ---\n",
    "train_folders = [\"2\", \"3\", \"4\"]\n",
    "train_folders = [\"1\"]\n",
    "val_folders = [\"1\"]\n",
    "test_folders = [\"5\"]\n",
    "\n",
    "train_img_folders = [os.path.join(BASE_DATA_DIR, f) for f in train_folders]\n",
    "val_img_folders = [os.path.join(BASE_DATA_DIR, f) for f in val_folders]\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_datasets = [DatasetDigitalStaining(folder, augmentation=None) for folder in train_img_folders]\n",
    "val_datasets = [DatasetDigitalStaining(folder, augmentation=None) for folder in val_img_folders]\n",
    "print(\"Dataset loading complete.\")\n",
    "\n",
    "# --- NEW: Calculate Global Statistics ---\n",
    "print(\"\\nCalculating global statistics from training set (this may take a moment)...\")\n",
    "ph1_all_pixels = []\n",
    "mito_all_pixels = []\n",
    "\n",
    "for dataset in train_datasets:\n",
    "    for ph1, ph2, mito in tqdm(dataset):\n",
    "        ph1_all_pixels.append(ph1.numpy().ravel())\n",
    "        mito_all_pixels.append(mito.numpy().ravel())\n",
    "\n",
    "# Concatenate all pixel values\n",
    "ph1_global_dist = np.concatenate(ph1_all_pixels)\n",
    "mito_global_dist = np.concatenate(mito_all_pixels)\n",
    "\n",
    "# Calculate percentiles\n",
    "ph1_stats = np.percentile(ph1_global_dist, [P_LOW, P_HIGH])\n",
    "mito_stats = np.percentile(mito_global_dist, [P_LOW, P_HIGH])\n",
    "\n",
    "print(f\"Global ph1 stats ({P_LOW}%, {P_HIGH}%): {ph1_stats[0]:.4f}, {ph1_stats[1]:.4f}\")\n",
    "print(f\"Global mito stats ({P_LOW}%, {P_HIGH}%): {mito_stats[0]:.2f}, {mito_stats[1]:.2f}\\n\")\n",
    "# --- End Statistics Calculation ---\n",
    "\n",
    "\n",
    "# --- Run the processing ---\n",
    "# Process the training datasets\n",
    "process_dataset(\n",
    "    datasets_list=train_datasets,\n",
    "    split_name=\"train\",\n",
    "    num_patches_per_image=NUM_PATCHES_PER_IMAGE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    base_dir=BASE_OUTPUT_DIR,\n",
    "    prompt=PROMPT,\n",
    "    ph1_stats=ph1_stats,   # Pass global stats\n",
    "    mito_stats=mito_stats  # Pass global stats\n",
    ")\n",
    "\n",
    "# Process the validation datasets\n",
    "# IMPORTANT: Use the *same stats from the training set*\n",
    "process_dataset(\n",
    "    datasets_list=val_datasets,\n",
    "    split_name=\"val\",\n",
    "    num_patches_per_image=NUM_PATCHES_PER_IMAGE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    base_dir=BASE_OUTPUT_DIR,\n",
    "    prompt=PROMPT,\n",
    "    ph1_stats=ph1_stats,   # Pass global stats from train\n",
    "    mito_stats=mito_stats  # Pass global stats from train\n",
    ")\n",
    "\n",
    "print(\"All processing complete.\")\n",
    "print(f\"Your ControlNet uint8 dataset is ready in: {BASE_OUTPUT_DIR}\")\n"
   ],
   "id": "e803effaf7b0c815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Dataset loading complete.\n",
      "\n",
      "Calculating global statistics from training set (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 139.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global ph1 stats (0.1%, 99.9%): -0.0079, 0.0092\n",
      "Global mito stats (0.1%, 99.9%): 0.00, 25959.00\n",
      "\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:02<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing train split.\n",
      "Saved 31 patches to: ..//controlnet_dataset_uint8\\train\n",
      "Saved metadata file to: ..//controlnet_dataset_uint8\\train\\metadata.jsonl\n",
      "\n",
      "\n",
      "Finished processing train split.\n",
      "Saved 31 patches to: ..//controlnet_dataset_uint8\\train\n",
      "Saved metadata file to: ..//controlnet_dataset_uint8\\train\\metadata.jsonl\n",
      "\n",
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:02<00:00, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing val split.\n",
      "Saved 31 patches to: ..//controlnet_dataset_uint8\\val\n",
      "Saved metadata file to: ..//controlnet_dataset_uint8\\val\\metadata.jsonl\n",
      "\n",
      "\n",
      "Finished processing val split.\n",
      "Saved 31 patches to: ..//controlnet_dataset_uint8\\val\n",
      "Saved metadata file to: ..//controlnet_dataset_uint8\\val\\metadata.jsonl\n",
      "\n",
      "All processing complete.\n",
      "Your ControlNet uint8 dataset is ready in: ..//controlnet_dataset_uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:39:01.905368Z",
     "start_time": "2025-10-22T11:39:01.903564Z"
    }
   },
   "cell_type": "code",
   "source": "# accelerate launch train_controlnet.py --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" --output_dir=\"D://Matsusaka//sd_outputs//mito_cn\" --train_data_dir=\"C://Users//Matsusaka//PycharmProjects//DDPM//controlnet_dataset_uint8//train\" --resolution=512 --learning_rate=1e-5 --train_batch_size=4 --num_train_epochs=100 --report_to=\"wandb\"",
   "id": "5f15c630fe982857",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:39:01.926430Z",
     "start_time": "2025-10-22T11:39:01.905940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# code from https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md for out data\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.data = []\n",
    "        self.folder = folder\n",
    "        fname = os.path.join(folder, \"metadata.jsonl\")\n",
    "        with open(fname, 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        image_f = item['file_name']\n",
    "        condition_f = item['conditioning_image']\n",
    "        prompt = item['text']\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.folder, image_f))\n",
    "        condition_img = cv2.imread(os.path.join(self.folder, condition_f))\n",
    "\n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        condition_img = cv2.cvtColor(condition_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        condition_img = condition_img.astype(np.float16) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        image = (image.astype(np.float16) / 127.5) - 1.0\n",
    "\n",
    "        return dict(jpg=image, txt=prompt, hint=condition_img)"
   ],
   "id": "75c07e57afefebec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:39:01.957328Z",
     "start_time": "2025-10-22T11:39:01.927060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder = \"C://Users//Matsusaka//PycharmProjects//DDPM//controlnet_dataset_uint8//train\"\n",
    "dataset = MyDataset(folder)\n",
    "print(len(dataset))\n",
    "\n",
    "item = dataset[0]\n",
    "jpg = item['jpg']\n",
    "txt = item['txt']\n",
    "hint = item['hint']\n",
    "print(txt)\n",
    "print(jpg.shape)\n",
    "print(hint.shape)"
   ],
   "id": "2b2657f092b1c3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "fluorescence microscopy image of mitochondria\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:45:31.188194Z",
     "start_time": "2025-10-22T11:43:13.158411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from cldm.logger import ImageLogger\n",
    "from cldm.model import create_model, load_state_dict\n",
    "\n",
    "\n",
    "model_f = \"D://Matsusaka//sd_outputs//control_v11f1p_sd15_depth.yaml\"\n",
    "# Configs\n",
    "resume_path = \"D://Matsusaka//sd_outputs//control_v11f1p_sd15_depth.pth\"\n",
    "batch_size = 4\n",
    "logger_freq = 300\n",
    "learning_rate = 1e-5\n",
    "sd_locked = True\n",
    "only_mid_control = False\n",
    "\n",
    "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
    "model = create_model(model_f).cpu()\n",
    "# model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n",
    "model.learning_rate = learning_rate\n",
    "model.sd_locked = sd_locked\n",
    "model.only_mid_control = only_mid_control\n",
    "\n",
    "# Misc\n",
    "dataset = MyDataset(folder)\n",
    "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "logger = ImageLogger(batch_frequency=logger_freq)\n",
    "trainer = pl.Trainer(precision=16, callbacks=[])\n",
    "\n",
    "\n",
    "# Train!\n",
    "trainer.fit(model, dataloader)"
   ],
   "id": "33002dbb73de6039",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config from [D://Matsusaka//sd_outputs//control_v11f1p_sd15_depth.yaml]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type               | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M  | train\n",
      "1 | first_stage_model | AutoencoderKL      | 83.7 M | eval \n",
      "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M  | eval \n",
      "3 | control_model     | ControlNet         | 361 M  | train\n",
      "-----------------------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "206 M     Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,710.058 Total estimated model params size (MB)\n",
      "1266      Modules in train mode\n",
      "365       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab5e9fb6c5fd467f99491db29f58a6a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a3fb8921eccca1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
