{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T11:08:20.968477Z",
     "start_time": "2025-10-10T11:08:20.956681Z"
    }
   },
   "source": [
    "import os\n",
    "from tifffile import tifffile\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_clip(img, vmin, vmax):\n",
    "    img = np.clip(img, vmin, vmax)\n",
    "    return (img - vmin) / (vmax - vmin)\n",
    "\n",
    "def standardization(img):\n",
    "    return (img - img.mean()) / img.std()\n",
    "\n",
    "def stand_norm(img, img_minmax):\n",
    "    img = standardization(img)\n",
    "    return normalize_clip(img, img_minmax[0], img_minmax[1])\n",
    "        \n",
    "def produce_images(original_dir, main_dir, data_folders=[\"1\", \"2\", \"3\", \"4\", \"5\"], train_idx=[1, 2, 3], val_test_idx=[0, 4],\n",
    "                   preprocess=True, img_n=100, img_size=256):\n",
    "    if preprocess:\n",
    "        original_img_folders = [os.path.join(original_dir, f) for f in [data_folders[k] for k in train_idx]]\n",
    "        img_list = []\n",
    "        for i in range(len(original_img_folders)):\n",
    "            img_path_list = os.listdir(original_img_folders[i])\n",
    "            for j in range(len(img_path_list)):\n",
    "                image = tifffile.imread(os.path.join(original_img_folders[i], img_path_list[j]))\n",
    "                img = []\n",
    "                for l in range(len(image[0][0])):\n",
    "                    img.append(standardization(image[..., l]).flatten())\n",
    "                img_list.append([img])\n",
    "        img_list = np.array(img_list)\n",
    "        img_minmax = []\n",
    "        for l in range(len(image[0][0])):\n",
    "            img_concat = np.concatenate(img_list[:, 0, l, :])\n",
    "            img_minmax.append([np.percentile(img_concat, 0.1), np.percentile(img_concat, 99.9)])\n",
    "        print(\"norm_measure done\")\n",
    "        \n",
    "    train_aug = A.Compose(\n",
    "        [\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.2, scale_limit=0.2,\n",
    "                rotate_limit=30, p=0.7,\n",
    "                border_mode=cv2.BORDER_REFLECT_101\n",
    "            ),\n",
    "            A.CropNonEmptyMaskIfExists(\n",
    "                height=img_size,\n",
    "                width=img_size,\n",
    "                p=1.0\n",
    "            ),\n",
    "        ],\n",
    "        additional_targets={\"image1\": \"image\",\n",
    "                            \"image2\": \"image\",\n",
    "                            \"image3\": \"image\"},\n",
    "        strict=True,\n",
    "        seed=137,\n",
    "    )\n",
    "    original_img_folders = [os.path.join(original_dir, f) for f in [data_folders[k] for k in train_idx]]\n",
    "    img_folders = [os.path.join(main_dir, f) for f in [data_folders[k] for k in train_idx]]\n",
    "    for i in range(len(original_img_folders)):\n",
    "        img_path_list = os.listdir(original_img_folders[i])\n",
    "        for j in range(len(img_path_list)):\n",
    "            image = tifffile.imread(os.path.join(original_img_folders[i],img_path_list[j]))\n",
    "            if preprocess:\n",
    "                for l in range(len(image[0][0])):\n",
    "                    image[..., l] = stand_norm(image[..., l], img_minmax[l])\n",
    "            phase1, phase2, mito = image[..., 0], image[..., 1], image[..., 2]\n",
    "            mask = (mito > 0).astype(np.float32)\n",
    "            for k in range(img_n):\n",
    "                augmented = train_aug(image=phase1, image1=phase2, image2=mito, mask=mask)\n",
    "                image_crop = np.stack([augmented['image'], augmented['image1'], augmented['image2']], axis=-1)\n",
    "                base_name, ext = os.path.splitext(img_path_list[j])\n",
    "                new_filename = f\"{base_name}_{k}{ext}\"\n",
    "                save_path = os.path.join(img_folders[i], new_filename)\n",
    "                tifffile.imwrite(save_path, image_crop)\n",
    "        print(f\"folder {img_folders[i]} done\")\n",
    "    \n",
    "    if preprocess:\n",
    "        original_img_folders = [os.path.join(original_dir, f) for f in [data_folders[k] for k in val_test_idx]]\n",
    "        img_folders = [os.path.join(main_dir, f) for f in [data_folders[k] for k in val_test_idx]]\n",
    "        for i in range(len(original_img_folders)):\n",
    "            img_path_list = os.listdir(original_img_folders[i])\n",
    "            for j in range(len(img_path_list)):\n",
    "                image = tifffile.imread(os.path.join(original_img_folders[i],img_path_list[j]))\n",
    "                for l in range(len(image[0][0])):\n",
    "                    image[..., l] = stand_norm(image[..., l], img_minmax[l])\n",
    "                tifffile.imwrite(os.path.join(img_folders[i],img_path_list[j]), image)\n",
    "            print(f\"folder {img_folders[i]} done\")\n",
    "\n",
    "\n",
    "def produce_images_2(original_dir, preprocess=True, img_n=100, img_size=256):\n",
    "    main_dir = original_dir\n",
    "    if preprocess:\n",
    "        original_img_folders = [os.path.join(original_dir, f) for f in ['1', '2', '3', '4', '5']]\n",
    "        img_list = []\n",
    "        for i in range(len(original_img_folders)):\n",
    "            img_path_list = os.listdir(original_img_folders[i])\n",
    "            for j in range(len(img_path_list)):\n",
    "                image = tifffile.imread(os.path.join(original_img_folders[i], img_path_list[j]))\n",
    "                img = []\n",
    "                for l in range(len(image[0][0])):\n",
    "                    img.append(standardization(image[..., l]).flatten())\n",
    "                img_list.append([img])\n",
    "        img_list = np.array(img_list)\n",
    "        img_minmax = []\n",
    "        for l in range(len(image[0][0])):\n",
    "            img_concat = np.concatenate(img_list[:, 0, l, :])\n",
    "            img_minmax.append([np.percentile(img_concat, 0.1), np.percentile(img_concat, 99.9)])\n",
    "        print(\"norm_measure done\")\n",
    "        \n",
    "    train_aug = A.Compose(\n",
    "        [\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.2, scale_limit=0.2,\n",
    "                rotate_limit=30, p=0.7,\n",
    "                border_mode=cv2.BORDER_REFLECT_101\n",
    "            ),\n",
    "            A.CropNonEmptyMaskIfExists(\n",
    "                height=img_size,\n",
    "                width=img_size,\n",
    "                p=1.0\n",
    "            ),\n",
    "        ],\n",
    "        additional_targets={\"image1\": \"image\",\n",
    "                            \"image2\": \"image\",\n",
    "                            \"image3\": \"image\"},\n",
    "        strict=True,\n",
    "        seed=137,\n",
    "    )\n",
    "    original_img_folders = [os.path.join(original_dir, f) for f in ['1', '2', '3', '4', '5']]\n",
    "    img_folders = [os.path.join(main_dir, f) for f in ['c1', 'c2', 'c3', 'c4', 'c5']]\n",
    "    for i in range(len(original_img_folders)):\n",
    "        img_path_list = os.listdir(original_img_folders[i])\n",
    "        for j in tqdm(range(len(img_path_list))):\n",
    "            ori_pth = os.path.join(original_img_folders[i],img_path_list[j])\n",
    "            image = tifffile.imread(ori_pth)\n",
    "            if preprocess:\n",
    "                for l in range(len(image[0][0])):\n",
    "                    image[..., l] = stand_norm(image[..., l], img_minmax[l])\n",
    "            phase1, phase2, mito = image[..., 0], image[..., 1], image[..., 2]\n",
    "            mask = (mito > 0).astype(np.float32)\n",
    "            tifffile.imwrite(ori_pth, image)\n",
    "            for k in range(img_n):\n",
    "                augmented = train_aug(image=phase1, image1=phase2, image2=mito, mask=mask)\n",
    "                image_crop = np.stack([augmented['image'], augmented['image1'], augmented['image2']], axis=-1)\n",
    "                base_name, ext = os.path.splitext(img_path_list[j])\n",
    "                new_filename = f\"{base_name}_{k}{ext}\"\n",
    "                save_path = os.path.join(img_folders[i], new_filename)\n",
    "                tifffile.imwrite(save_path, image_crop)\n",
    "        print(f\"folder {img_folders[i]} done\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T11:23:27.514293Z",
     "start_time": "2025-10-10T11:08:22.166825Z"
    }
   },
   "cell_type": "code",
   "source": "produce_images_2(r\"D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\")",
   "id": "c8715abd9b0d6d5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matsusaka\\PycharmProjects\\DigitalStainingGAN2\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_measure done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:15<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\\c1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\\c2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:27<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\\c3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:43<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\\c4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\\c5 done\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:05:44.048112Z",
     "start_time": "2025-10-10T11:52:34.598648Z"
    }
   },
   "cell_type": "code",
   "source": "produce_images_2(r\"D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\")",
   "id": "7f47385bc97dfa92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_measure done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:01<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\\c1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:40<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\\c2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:54<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\\c3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:02<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\\c4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder D:\\Matsusaka\\data_mito\\HeLa_Su9-mSG_kmeans\\c5 done\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "produce_images(\"D:\\\\Matsusaka\\\\data_mito\\\\COS7_KDEL-mSG_UNetPipeline\", \"D:\\\\Matsusaka\\\\data_mito\\\\COS7_KDEL-mSG_UNetPipeline_crop\",\n",
    "               data_folders=[\"1\", \"2\", \"3\"], train_idx=[0], val_test_idx=[1,2], preprocess=False)\n",
    "produce_images(\"D:\\\\Matsusaka\\\\data_mito\\\\COS7_KDEL-mSG\", \"D:\\\\Matsusaka\\\\data_mito\\\\COS7_KDEL-mSG_original_crop\",\n",
    "               data_folders=[\"1\", \"2\", \"3\"], train_idx=[0], val_test_idx=[1,2], preprocess=True)"
   ],
   "id": "a26c9fdc24cbe596"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T11:00:05.211111Z",
     "start_time": "2025-10-10T11:00:05.207048Z"
    }
   },
   "cell_type": "code",
   "source": "os.listdir(r\"D:\\Matsusaka\\data_mito\\COS7_KDEL-mSG_kmeans\")",
   "id": "126e926961f11f4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', 'c1', 'c2', 'c3', 'c4', 'c5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd28fd14348762c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
